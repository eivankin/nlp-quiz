[
  {
    "id": 1,
    "question": "Сколько памяти потребуется для обучения 7B модели в формате FP32 с использованием оптимизатора Momentum, если сама модель требует 28 ГБ для хранения параметров?",
    "type": "single",
    "options": [
      "28 ГБ",
      "56 ГБ",
      "84 ГБ",
      "112 ГБ"
    ],
    "correct": [
      2
    ]
  },
  {
    "id": 2,
    "question": "Сколько памяти потребуется для обучения 7B модели в формате 4-bit с использованием оптимизатора RMSProp, если параметры занимают 3.5 ГБ?",
    "type": "single",
    "options": [
      "3.5 ГБ",
      "7 ГБ",
      "10.5 ГБ",
      "14 ГБ"
    ],
    "correct": [
      2
    ]
  },
  {
    "id": 3,
    "question": "Сколько памяти потребуется для хранения параметров, градиентов, экспоненциального среднего градиентов (m) и среднего квадратов градиентов (v) для 7B модели в FP16 с оптимизатором Adam?",
    "type": "single",
    "options": [
      "14 ГБ",
      "28 ГБ",
      "56 ГБ",
      "84 ГБ"
    ],
    "correct": [
      2
    ]
  },
  {
    "id": 4,
    "question": "Какое основное отличие в потреблении памяти для 7B модели в формате 8-bit при инференсе по сравнению с обучением с оптимизатором Adagrad?",
    "type": "single",
    "options": [
      "При инференсе хранятся только параметры модели",
      "При обучении не используются градиенты",
      "Инференс требует больше памяти из-за KV cache",
      "Adagrad не хранит дополнительные состояния"
    ],
    "correct": [
      0
    ]
  },
  {
    "id": 5,
    "question": "Какая функция из библиотеки Transformers используется для загрузки предобученной модели?",
    "type": "single",
    "options": [
      "load_model",
      "AutoModel.from_pretrained",
      "Trainer.load",
      "ModelHub.download"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 6,
    "question": "Какой тип токенизации поддерживается библиотекой Tokenizers, чтобы эффективно работать с низкоресурсными языками?",
    "type": "single",
    "options": [
      "Word-level tokenization",
      "Character-level tokenization",
      "Subword tokenization",
      "Sentence-level tokenization"
    ],
    "correct": [
      2
    ]
  },
  {
    "id": 7,
    "question": "Какой метод из библиотеки PEFT используется для адаптации модели с помощью параметров низкого ранга?",
    "type": "single",
    "options": [
      "Prefix Tuning",
      "Adapter Tuning",
      "LoRA",
      "Prompt Tuning"
    ],
    "correct": [
      2
    ]
  },
  {
    "id": 8,
    "question": "Какой метод библиотеки TRL используется для реализации обучения с подкреплением от человеческой обратной связи (RLHF)?",
    "type": "single",
    "options": [
      "DPO",
      "PPO",
      "REINFORCE",
      "Actor-Critic"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 9,
    "question": "Какая функция в библиотеке Sentence Transformers используется для вычисления эмбеддингов текста?",
    "type": "single",
    "options": [
      "encode",
      "embed",
      "forward",
      "vectorize"
    ],
    "correct": [
      0
    ]
  },
  {
    "id": 10,
    "question": "Какая функция модуля re используется для поиска всех непересекающихся совпадений шаблона в строке?",
    "type": "single",
    "options": [
      "re.search",
      "re.match",
      "re.findall",
      "re.finditer"
    ],
    "correct": [
      2
    ]
  },
  {
    "id": 11,
    "question": "Какой метод нормализации используется для преобразования текста в нижний регистр?",
    "type": "single",
    "options": [
      "strip",
      "lower",
      "normalize",
      "casefold"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 12,
    "question": "Какая библиотека Python чаще всего используется для выполнения стемминга?",
    "type": "single",
    "options": [
      "spaCy",
      "NLTK",
      "Transformers",
      "Gensim"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 13,
    "question": "Какой из приведённых типов токенизации основан на разбиении текста на предложения?",
    "type": "single",
    "options": [
      "Word tokenization",
      "Sentence tokenization",
      "Subword tokenization",
      "Character tokenization"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 14,
    "question": "Какой шаг выполняется в алгоритме Byte-Pair Encoding (BPE)?",
    "type": "single",
    "options": [
      "Случайное объединение токенов",
      "Объединение наиболее частой пары символов",
      "Удаление редких слов",
      "Оптимизация длины предложения"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 15,
    "question": "Какой метод токенизации реализован в библиотеке SentencePiece?",
    "type": "multiple",
    "options": [
      "BPE",
      "Unigram Language Model",
      "WordPiece",
      "Character-level tokenization"
    ],
    "correct": [
      0,
      1
    ]
  },
  {
    "id": 16,
    "question": "Какой принцип лежит в основе алгоритма WordPiece?",
    "type": "single",
    "options": [
      "Минимизация словаря символов",
      "Максимизация правдоподобия субслов",
      "Объединение частых пар байтов",
      "Случайная сегментация слов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 17,
    "question": "Какая из следующих характеристик описывает One-Hot Encoding?",
    "type": "multiple",
    "options": [
      "Низкая размерность",
      "Разреженное представление",
      "Учет семантической близости",
      "Наличие информации о порядке"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 18,
    "question": "Какую информацию сохраняет модель Bag of Words?",
    "type": "multiple",
    "options": [
      "Частоту слов",
      "Порядок слов",
      "Наличие слов",
      "Синтаксическую структуру"
    ],
    "correct": [
      0,
      2
    ]
  },
  {
    "id": 19,
    "question": "Какая из следующих задач может быть решена с помощью Naive Bayes?",
    "type": "single",
    "options": [
      "Кластеризация",
      "Классификация текста",
      "Генерация текста",
      "Поиск ближайших соседей"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 20,
    "question": "Что означает компонент IDF в термине TF-IDF?",
    "type": "single",
    "options": [
      "Частоту термина в документе",
      "Обратную частоту документа",
      "Количество документов",
      "Нормализацию длины текста"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 21,
    "question": "Что измеряет Cosine Similarity?",
    "type": "single",
    "options": [
      "Евклидово расстояние",
      "Угол между векторами",
      "Длину вектора",
      "Разность координат"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 22,
    "question": "Какая основная задача решается с помощью n-граммных моделей?",
    "type": "single",
    "options": [
      "Семантический поиск",
      "Моделирование вероятности последовательностей",
      "Кластеризация текстов",
      "Обучение эмбеддингов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 23,
    "question": "Что измеряет функция потерь Cross-Entropy?",
    "type": "single",
    "options": [
      "Расстояние между векторами",
      "Разницу предсказаний и истинных меток",
      "Сходство документов",
      "Дисперсию признаков"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 24,
    "question": "Что характеризует Perplexity в языковых моделях?",
    "type": "single",
    "options": [
      "Скорость обучения",
      "Неопределенность предсказаний модели",
      "Размер словаря",
      "Длину последовательности"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 25,
    "question": "В каких задачах используется BLEU Score?",
    "type": "single",
    "options": [
      "Классификация текста",
      "Машинный перевод",
      "Семантический поиск",
      "Кластеризация"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 26,
    "question": "Для какой задачи применяется ROUGE Score?",
    "type": "single",
    "options": [
      "Машинный перевод",
      "Суммаризация текста",
      "Классификация",
      "Извлечение сущностей"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 27,
    "question": "Как интерпретируется метрика Precision?",
    "type": "single",
    "options": [
      "Доля найденных релевантных объектов от всех релевантных",
      "Доля релевантных объектов среди найденных",
      "Среднее гармоническое Precision и Recall",
      "Вероятность правильной классификации"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 28,
    "question": "Что измеряет метрика Recall?",
    "type": "single",
    "options": [
      "Долю правильно классифицированных объектов среди всех объектов",
      "Долю релевантных объектов, которые были обнаружены моделью",
      "Долю обнаруженных объектов, которые являются релевантными",
      "Среднее значение Precision и Recall"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 29,
    "question": "Как определяется F1 Score?",
    "type": "single",
    "options": [
      "Среднее арифметическое Precision и Recall",
      "Среднее геометрическое Precision и Recall",
      "Гармоническое среднее Precision и Recall",
      "Разность Precision и Recall"
    ],
    "correct": [
      2
    ]
  },
  {
    "id": 30,
    "question": "В чем основное назначение функции потерь Cross-Entropy Loss?",
    "type": "single",
    "options": [
      "Измерение сходства векторов",
      "Оценка расхождения между предсказанным и истинным распределением",
      "Снижение размерности",
      "Регуляризация модели"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 31,
    "question": "Какова основная цель Backpropagation в нейронных сетях?",
    "type": "single",
    "options": [
      "Инициализация весов",
      "Обновление весов на основе градиентов",
      "Снижение переобучения",
      "Ускорение инференса"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 32,
    "question": "Как используется Chain Rule в алгоритме Backpropagation?",
    "type": "single",
    "options": [
      "Для вычисления прямого прохода",
      "Для последовательного вычисления градиентов слоев",
      "Для нормализации весов",
      "Для обрезки градиентов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 33,
    "question": "Какой из следующих методов оптимизации используется в глубоком обучении?",
    "type": "multiple",
    "options": [
      "SGD",
      "Adam",
      "RMSProp",
      "k-means"
    ],
    "correct": [
      0,
      1,
      2
    ]
  },
  {
    "id": 34,
    "question": "Чем Adam отличается от стандартного градиентного спуска?",
    "type": "multiple",
    "options": [
      "Использует адаптивные learning rate",
      "Хранит экспоненциальные средние градиентов",
      "Не использует градиенты",
      "Комбинирует идеи Momentum и RMSProp"
    ],
    "correct": [
      0,
      1,
      3
    ]
  },
  {
    "id": 35,
    "question": "Какую функцию активации предпочтительно использовать для нейронов скрытых слоев?",
    "type": "single",
    "options": [
      "Sigmoid",
      "Softmax",
      "ReLU",
      "Linear"
    ],
    "correct": [
      2
    ]
  },
  {
    "id": 36,
    "question": "Какую функцию потерь обычно используют для задач классификации?",
    "type": "single",
    "options": [
      "Mean Squared Error",
      "Cross-Entropy Loss",
      "Huber Loss",
      "Cosine Loss"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 37,
    "question": "Какое преимущество дают сверточные нейронные сети (CNN) в задачах обработки естественного языка (NLP)?",
    "type": "multiple",
    "options": [
      "Извлечение локальных n-граммных признаков",
      "Параллельная обработка последовательностей",
      "Учет глобального контекста без ограничений",
      "Меньшая чувствительность к позиции признаков"
    ],
    "correct": [
      0,
      1,
      3
    ]
  },
  {
    "id": 38,
    "question": "Какая идея лежит в основе Word2Vec?",
    "type": "single",
    "options": [
      "Предсказание следующего слова",
      "Совместная частота слов",
      "Контекстное предсказание слов",
      "Оптимизация расстояния между предложениями"
    ],
    "correct": [
      2
    ]
  },
  {
    "id": 39,
    "question": "Чем GloVe отличается от Word2Vec?",
    "type": "single",
    "options": [
      "Использует только локальный контекст",
      "Основан на глобальных статистиках совместной встречаемости",
      "Является контекстуальной моделью",
      "Не использует матричное представление"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 40,
    "question": "Какое ключевое отличие модели FastText от Word2Vec?",
    "type": "single",
    "options": [
      "Использование символных n-грамм",
      "Контекстуальные эмбеддинги",
      "Обучение без корпуса",
      "Использование attention"
    ],
    "correct": [
      0
    ]
  },
  {
    "id": 41,
    "question": "Какова основная проблема Vanilla RNN?",
    "type": "single",
    "options": [
      "Большое количество параметров",
      "Исчезающие и взрывающиеся градиенты",
      "Невозможность обучения",
      "Отсутствие нелинейностей"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 42,
    "question": "Чем LSTM (Long Short-Term Memory) отличается от стандартных RNN?",
    "type": "multiple",
    "options": [
      "Использует gating-механизмы",
      "Лучше сохраняет долгосрочные зависимости",
      "Имеет меньше параметров",
      "Решает проблему исчезающих градиентов"
    ],
    "correct": [
      0,
      1,
      3
    ]
  },
  {
    "id": 43,
    "question": "Какую дополнительную информацию дает Bidirectional LSTM?",
    "type": "single",
    "options": [
      "Контекст только слева",
      "Контекст только справа",
      "Контекст с обеих сторон",
      "Глобальную память"
    ],
    "correct": [
      2
    ]
  },
  {
    "id": 44,
    "question": "Чем GRU (Gated Recurrent Unit) отличается от LSTM?",
    "type": "single",
    "options": [
      "Использует больше gating-механизмов",
      "Имеет более простую архитектуру",
      "Не может моделировать долгосрочные зависимости",
      "Использует attention"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 45,
    "question": "Какую основную идею реализует модель ELMo?",
    "type": "single",
    "options": [
      "Статические эмбеддинги слов",
      "Контекстуальные эмбеддинги",
      "Субсловную токенизацию",
      "Авто-регрессионное моделирование"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 46,
    "question": "Какова основная цель RNN-based Sequence-to-Sequence (Seq2Seq) Model?",
    "type": "single",
    "options": [
      "Классификация последовательностей",
      "Преобразование одной последовательности в другую",
      "Кластеризация предложений",
      "Извлечение признаков"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 47,
    "question": "Зачем используется Attention Mechanism в Seq2Seq моделях?",
    "type": "multiple",
    "options": [
      "Фокусировка на релевантных частях входа",
      "Устранение необходимости encoder",
      "Улучшение обработки длинных последовательностей",
      "Снижение размерности входа"
    ],
    "correct": [
      0,
      2
    ]
  },
  {
    "id": 48,
    "question": "Что является ключевой особенностью Bahdanau Attention?",
    "type": "single",
    "options": [
      "Использование скалярного произведения",
      "Аддитивная форма вычисления внимания",
      "Отсутствие параметров",
      "Использование masked attention"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 49,
    "question": "Чем Luong Attention отличается от Bahdanau Attention?",
    "type": "single",
    "options": [
      "Использует аддитивное внимание",
      "Использует multiplicative (dot-product) внимание",
      "Работает только в трансформерах",
      "Не использует encoder hidden states"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 50,
    "question": "Какое преимущество добавляет Attention к стандартной Seq2Seq модели?",
    "type": "multiple",
    "options": [
      "Фокусировку на релевантных частях входной последовательности",
      "Устранение необходимости encoder",
      "Лучшее моделирование длинных последовательностей",
      "Снижение вычислительной сложности"
    ],
    "correct": [
      0,
      2
    ]
  },
  {
    "id": 51,
    "question": "Какое отличие Transformer Encoder-Decoder Architecture от Seq2Seq на основе RNN?",
    "type": "single",
    "options": [
      "Использование рекуррентных связей",
      "Параллельная обработка последовательностей",
      "Отсутствие attention механизмов",
      "Обучение только в autoregressive режиме"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 52,
    "question": "Какую задачу решает Transformer Encoder?",
    "type": "single",
    "options": [
      "Генерацию выходной последовательности",
      "Построение контекстных представлений входа",
      "Выбор следующего токена",
      "Сэмплирование токенов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 53,
    "question": "Какую роль выполняет Transformer Decoder?",
    "type": "single",
    "options": [
      "Кодирование входной последовательности",
      "Генерация выходной последовательности авторегрессионно",
      "Кластеризация токенов",
      "Обучение эмбеддингов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 54,
    "question": "Какова основная цель механизма Self-Attention?",
    "type": "single",
    "options": [
      "Сравнение разных предложений",
      "Моделирование зависимостей между токенами внутри одной последовательности",
      "Уменьшение размерности эмбеддингов",
      "Генерация позиционных кодов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 55,
    "question": "Чем отличается Cross-Attention от Self-Attention?",
    "type": "single",
    "options": [
      "Использует разные последовательности для queries и keys/values",
      "Работает только в encoder",
      "Не использует learnable параметры",
      "Применяется только при инференсе"
    ],
    "correct": [
      0
    ]
  },
  {
    "id": 56,
    "question": "Какую проблему решает Multi-Head Attention?",
    "type": "single",
    "options": [
      "Снижение переобучения",
      "Захват разных типов зависимостей в разных подпространствах",
      "Уменьшение количества параметров",
      "Ускорение обучения за счет рекурсии"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 57,
    "question": "Какова основная цель использования Sliding Window Attention?",
    "type": "single",
    "options": [
      "Учет глобального контекста всей последовательности",
      "Снижение вычислительной и памятйной сложности для длинных последовательностей",
      "Повышение качества генерации",
      "Замена positional encoding"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 58,
    "question": "Для чего используется Layer Normalization в трансформерах?",
    "type": "single",
    "options": [
      "Нормализация по батчу",
      "Стабилизация и ускорение обучения",
      "Сжатие модели",
      "Регуляризация весов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 59,
    "question": "Какую функцию выполняют Residual Connections в трансформерах?",
    "type": "multiple",
    "options": [
      "Облегчают прохождение градиентов",
      "Ускоряют сходимость обучения",
      "Полностью устраняют vanishing gradients",
      "Позволяют обучать более глубокие модели"
    ],
    "correct": [
      0,
      1,
      3
    ]
  },
  {
    "id": 60,
    "question": "Какова основная цель задачи Masked Language Modeling (MLM)?",
    "type": "single",
    "options": [
      "Генерация текста слева направо",
      "Предсказание замаскированных токенов по контексту",
      "Ранжирование предложений",
      "Оптимизация perplexity"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 61,
    "question": "Какой компонент обычно добавляется к трансформеру для выполнения классификации?",
    "type": "single",
    "options": [
      "Decoder block",
      "Pooling / classification head",
      "KV cache",
      "Attention mask"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 62,
    "question": "Какую роль играют Sampling Strategies при генерации текста?",
    "type": "single",
    "options": [
      "Определяют архитектуру модели",
      "Контролируют выбор следующего токена",
      "Используются только при обучении",
      "Снижают размер модели"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 63,
    "question": "Как работает Greedy Sampling?",
    "type": "single",
    "options": [
      "Случайно выбирает токен",
      "Всегда выбирает токен с максимальной вероятностью",
      "Выбирает токены из top-k",
      "Использует beam search"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 64,
    "question": "Чем Stochastic Sampling отличается от Greedy Sampling?",
    "type": "single",
    "options": [
      "Использует только один путь генерации",
      "Выбирает токены случайно с учетом распределения вероятностей",
      "Не использует softmax",
      "Работает только с beam search"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 65,
    "question": "Какова основная идея Top-k Sampling?",
    "type": "single",
    "options": [
      "Выбор токена с максимальной вероятностью",
      "Сэмплирование только из k наиболее вероятных токенов",
      "Сэмплирование до достижения суммарной вероятности p",
      "Детерминированная генерация"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 66,
    "question": "Что контролирует параметр p в Top-p Sampling?",
    "type": "single",
    "options": [
      "Количество кандидатов",
      "Суммарную вероятность выбранных токенов",
      "Температуру распределения",
      "Длину последовательности"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 67,
    "question": "Как параметр Temperature влияет на процесс генерации текста?",
    "type": "single",
    "options": [
      "Изменяет архитектуру модели",
      "Контролирует степень случайности распределения вероятностей",
      "Определяет длину генерации",
      "Включает или отключает sampling"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 68,
    "question": "Для чего используются Decoding Strategies?",
    "type": "single",
    "options": [
      "Для обучения модели",
      "Для выбора токенов при генерации",
      "Для токенизации входного текста",
      "Для оптимизации памяти"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 69,
    "question": "Какой недостаток может возникнуть при использовании Greedy Search?",
    "type": "single",
    "options": [
      "Высокая вычислительная сложность",
      "Застревание в локально оптимальных решениях",
      "Невозможность генерации длинных последовательностей",
      "Переобучение модели"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 70,
    "question": "Как Beam Search улучшает генерацию текста?",
    "type": "single",
    "options": [
      "Гарантирует глобально оптимальное решение",
      "Поддерживает несколько наиболее вероятных гипотез",
      "Использует случайное сэмплирование",
      "Снижает потребление памяти"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 71,
    "question": "Какова основная цель использования Beam-search Multinomial Sampling?",
    "type": "single",
    "options": [
      "Полностью детерминированная генерация",
      "Комбинация разнообразия и качества генерации",
      "Снижение сложности beam search",
      "Работа без softmax"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 72,
    "question": "Как Diverse Beam Search отличается от стандартного Beam Search?",
    "type": "single",
    "options": [
      "Использует случайное сэмплирование",
      "Поощряет разнообразие между гипотезами",
      "Работает только для диалогов",
      "Использует один beam"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 73,
    "question": "Как Speculative Decoding помогает ускорить генерацию текста?",
    "type": "single",
    "options": [
      "За счет уменьшения размера модели",
      "Используя вспомогательную модель для предложения токенов",
      "За счет отключения attention",
      "Используя только greedy decoding"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 74,
    "question": "Что подразумевается под Assisted Generation?",
    "type": "single",
    "options": [
      "Генерация текста без модели",
      "Использование дополнительной модели или механизма для ускорения/улучшения генерации",
      "Ручной выбор токенов",
      "Генерация только с temperature = 0"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 75,
    "question": "Какую зависимость описывают Scaling Laws?",
    "type": "single",
    "options": [
      "Зависимость скорости инференса от архитектуры",
      "Связь качества модели с размером данных, модели и вычислений",
      "Зависимость качества от learning rate",
      "Связь между batch size и переобучением"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 76,
    "question": "Каковы ключевые особенности In-context Learning?",
    "type": "multiple",
    "options": [
      "Обучение без обновления весов модели",
      "Использование примеров в prompt",
      "Требует дополнительного fine-tuning",
      "Зависит от масштаба модели"
    ],
    "correct": [
      0,
      1,
      3
    ]
  },
  {
    "id": 77,
    "question": "В чем состоит ключевая идея Instruction Following?",
    "type": "single",
    "options": [
      "Обучение модели только на размеченных данных",
      "Обучение модели следовать текстовым инструкциям",
      "Использование reinforcement learning",
      "Обучение без контекста"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 78,
    "question": "Почему Step-by-Step Reasoning важен для моделей NLP?",
    "type": "single",
    "options": [
      "Снижает размер модели",
      "Улучшает способность решать сложные многошаговые задачи",
      "Ускоряет инференс",
      "Устраняет необходимость данных"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 79,
    "question": "Какова основная архитектурная особенность BERT?",
    "type": "single",
    "options": [
      "Только decoder архитектура",
      "Двунаправленный Transformer Encoder",
      "Автогрегрессионное обучение",
      "Использование RNN"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 80,
    "question": "Какое улучшение предложил ALBERT по сравнению с BERT?",
    "type": "multiple",
    "options": [
      "Factorized embeddings",
      "Parameter sharing между слоями",
      "Использование decoder",
      "Отказ от self-attention"
    ],
    "correct": [
      0,
      1
    ]
  },
  {
    "id": 81,
    "question": "Какова основная задача обучения GPT?",
    "type": "single",
    "options": [
      "Предсказание замаскированных токенов",
      "Автогрегрессионное предсказание следующего токена",
      "Классификация предложений",
      "Ранжирование документов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 82,
    "question": "В чем особенность модели T5?",
    "type": "single",
    "options": [
      "Использует только encoder",
      "Использует только decoder",
      "Формулирует все задачи как text-to-text",
      "Обучается только в zero-shot режиме"
    ],
    "correct": [
      2
    ]
  },
  {
    "id": 83,
    "question": "Как XLNet улучшает двунаправленное моделирование по сравнению с BERT?",
    "type": "single",
    "options": [
      "Использует masked language modeling",
      "Использует permutation-based training",
      "Использует encoder-decoder архитектуру",
      "Не использует positional encoding"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 84,
    "question": "В чем заключается основная идея Absolute Positional Encoding в трансформерах?",
    "type": "single",
    "options": [
      "Кодирование относительных расстояний между токенами",
      "Добавление фиксированной или обучаемой позиции каждому токену",
      "Использование рекуррентных связей",
      "Динамическое изменение позиции"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 85,
    "question": "Чем Relative Positional Encodings отличаются от Absolute Positional Encoding?",
    "type": "single",
    "options": [
      "Кодируют абсолютный индекс токена",
      "Учитывают относительное положение токенов друг относительно друга",
      "Используются только в encoder",
      "Не влияют на attention"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 86,
    "question": "Какая ключевая характеристика Rotary Position Embeddings?",
    "type": "single",
    "options": [
      "Добавление фиксированных синусоидальных векторов",
      "Вращение query и key в комплексном пространстве",
      "Использование отдельного embedding слоя",
      "Работа только в decoder"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 87,
    "question": "Зачем используется Parameter Sharing в глубоких моделях?",
    "type": "multiple",
    "options": [
      "Снижение числа параметров",
      "Улучшение обобщающей способности",
      "Увеличение вычислительной сложности",
      "Снижение требований к памяти"
    ],
    "correct": [
      0,
      1,
      3
    ]
  },
  {
    "id": 88,
    "question": "Что добавляет Multi/Grouped Query Attention по сравнению с обычным механизмом внимания?",
    "type": "single",
    "options": [
      "Отдельные key и value для каждой головы",
      "Разделение query и общие key/value",
      "Полный отказ от attention",
      "Использование рекуррентных слоев"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 89,
    "question": "Как KV Cache помогает ускорить генерацию текста?",
    "type": "single",
    "options": [
      "Сохраняет только embeddings",
      "Избегает повторного вычисления key и value для предыдущих токенов",
      "Снижает размер модели",
      "Используется только при обучении"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 90,
    "question": "В чем ключевое преимущество FlashAttention?",
    "type": "single",
    "options": [
      "Снижение числа параметров",
      "Более эффективное использование памяти и вычислений",
      "Улучшение качества модели",
      "Отказ от softmax"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 91,
    "question": "Какова основная цель PagedAttention?",
    "type": "single",
    "options": [
      "Сжатие весов модели",
      "Эффективное управление памятью KV cache",
      "Замена positional encoding",
      "Ускорение обучения"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 92,
    "question": "Как работает подход Mixture of Experts?",
    "type": "single",
    "options": [
      "Все эксперты активны одновременно",
      "Для каждого входа активируется подмножество экспертов",
      "Используется только один эксперт",
      "Эксперты обучаются независимо без маршрутизации"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 93,
    "question": "Какова основная идея Knowledge Distillation?",
    "type": "single",
    "options": [
      "Обучение модели без данных",
      "Передача знаний от большой модели к меньшей",
      "Совместное обучение нескольких моделей",
      "Удаление параметров модели"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 94,
    "question": "Что подразумевается под Distributed Training в контексте глубокого обучения?",
    "type": "single",
    "options": [
      "Обучение модели на одном GPU",
      "Распределенное обучение на нескольких вычислительных устройствах",
      "Обучение без синхронизации",
      "Использование только CPU"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 95,
    "question": "Какова цель Data Parallelism в распределенном обучении?",
    "type": "single",
    "options": [
      "Разделение модели по слоям",
      "Параллельная обработка разных батчей данных",
      "Разделение матриц весов",
      "Последовательная обработка данных"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 96,
    "question": "В чем заключается принцип Pipeline (Model) Parallelism?",
    "type": "single",
    "options": [
      "Разделение данных между устройствами",
      "Разделение модели на последовательные части",
      "Дублирование модели на каждом устройстве",
      "Случайное распределение слоев"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 97,
    "question": "Как работает Tensor Parallelism?",
    "type": "single",
    "options": [
      "Разделяет данные между устройствами",
      "Разделяет тензоры весов между устройствами",
      "Использует только CPU",
      "Работает без синхронизации"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 98,
    "question": "Что такое Mixed Precision Training?",
    "type": "single",
    "options": [
      "Использование только FP32",
      "Комбинация разных форматов чисел для ускорения и экономии памяти",
      "Обучение без градиентов",
      "Использование квантованных весов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 99,
    "question": "Что такое Gradient Accumulation and Synchronization?",
    "type": "single",
    "options": [
      "Обновление весов после каждого примера",
      "Накопление градиентов перед обновлением весов и их синхронизация",
      "Удаление градиентов",
      "Только синхронизация весов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 100,
    "question": "Для чего используется All-Reduce for Gradient Aggregation в распределенном обучении?",
    "type": "single",
    "options": [
      "Для уменьшения размера модели",
      "Для агрегации градиентов между устройствами",
      "Для сжатия данных",
      "Для ускорения инференса"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 101,
    "question": "Что такое Quantization в контексте машинного обучения?",
    "type": "single",
    "options": [
      "Удаление параметров модели",
      "Снижение точности представления весов и активаций",
      "Увеличение размерности модели",
      "Оптимизация архитектуры"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 102,
    "question": "Что такое Post-Training Quantization?",
    "type": "single",
    "options": [
      "Квантование во время обучения",
      "Квантование уже обученной модели",
      "Совместное обучение нескольких моделей",
      "Удаление слоев модели"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 103,
    "question": "Как работает Quantization-Aware Training?",
    "type": "single",
    "options": [
      "Модель обучается без квантования",
      "Квантование имитируется во время обучения",
      "Квантование применяется только при инференсе",
      "Используется только для весов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 104,
    "question": "Что такое Distillation в контексте машинного обучения?",
    "type": "single",
    "options": [
      "Удаление параметров модели",
      "Передача знаний от teacher модели к student модели",
      "Обучение без данных",
      "Регуляризация модели"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 105,
    "question": "Что такое Logit-Based Distillation?",
    "type": "single",
    "options": [
      "Передача промежуточных активаций",
      "Обучение student модели на soft logits teacher модели",
      "Совместное обучение teacher и student",
      "Квантование логитов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 106,
    "question": "В чем заключается принцип Feature-Based Distillation?",
    "type": "single",
    "options": [
      "Сравнение выходных логитов",
      "Сопоставление промежуточных представлений teacher и student",
      "Удаление скрытых слоев",
      "Снижение размерности входа"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 107,
    "question": "Что подразумевается под Relational Distillation?",
    "type": "single",
    "options": [
      "Сравнение отдельных выходов моделей",
      "Сохранение отношений между примерами или представлениями",
      "Передача весов модели",
      "Использование reinforcement learning"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 108,
    "question": "Что такое Pruning в контексте нейронных сетей?",
    "type": "single",
    "options": [
      "Добавление новых параметров",
      "Удаление или зануление малозначимых параметров",
      "Квантование весов",
      "Снижение learning rate"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 109,
    "question": "Что такое Corpora for Pre-training?",
    "type": "single",
    "options": [
      "Наборы данных для оценки моделей",
      "Большие текстовые наборы для предварительного обучения моделей",
      "Данные для RLHF",
      "Синтетические данные для инференса"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 110,
    "question": "Что такое Instruction Tuning Datasets?",
    "type": "single",
    "options": [
      "Наборы данных без инструкций",
      "Данные для обучения моделей следовать инструкциям",
      "Данные только для классификации",
      "Корпуса без разметки"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 111,
    "question": "В чем заключается назначение Alignment Datasets?",
    "type": "single",
    "options": [
      "Обучение моделей генерации кода",
      "Согласование поведения модели с человеческими предпочтениями",
      "Увеличение размера словаря",
      "Снижение времени инференса"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 112,
    "question": "Какую роль выполняет Filtering and Selection в подготовке данных для обучения?",
    "type": "multiple",
    "options": [
      "Удаление шумных и нерелевантных данных",
      "Повышение качества обучающих данных",
      "Увеличение объема данных",
      "Снижение риска обучения на нежелательном контенте"
    ],
    "correct": [
      0,
      1,
      3
    ]
  },
  {
    "id": 113,
    "question": "Что подразумевается под De-duplication в обработке данных?",
    "type": "single",
    "options": [
      "Добавление новых данных",
      "Удаление дублирующихся примеров",
      "Сжатие данных",
      "Шифрование данных"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 114,
    "question": "Что означает Privacy Reduction в контексте обработки данных?",
    "type": "single",
    "options": [
      "Удаление персональных данных и чувствительной информации",
      "Сжатие данных",
      "Увеличение прозрачности модели",
      "Удаление метаданных"
    ],
    "correct": [
      0
    ]
  },
  {
    "id": 115,
    "question": "Что такое Data Scheduling for Pre-training LLMs?",
    "type": "single",
    "options": [
      "Фиксированный порядок данных без изменений",
      "Стратегия изменения состава или порядка данных во времени обучения",
      "Удаление данных после обучения",
      "Обучение на одном домене"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 116,
    "question": "Что такое Data Mixture в контексте обучения больших моделей?",
    "type": "single",
    "options": [
      "Использование одного источника данных",
      "Комбинация данных из разных источников",
      "Удаление нерелевантных данных",
      "Обучение без данных"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 117,
    "question": "Что такое Data Curriculum в обучении моделей?",
    "type": "single",
    "options": [
      "Случайная подача данных",
      "Подача данных от простых к более сложным",
      "Использование только сложных примеров",
      "Обучение без расписания"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 118,
    "question": "Что такое DeepSpeed lib?",
    "type": "single",
    "options": [
      "Библиотека для инференса моделей",
      "Фреймворк для эффективного распределенного обучения больших моделей",
      "Инструмент для токенизации",
      "Система мониторинга"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 119,
    "question": "Что такое Megatron-LM?",
    "type": "single",
    "options": [
      "Библиотека для визуализации",
      "Фреймворк для обучения очень больших трансформеров",
      "Инструмент для инференса",
      "Dataset"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 120,
    "question": "Что такое Colossal-AI?",
    "type": "single",
    "options": [
      "Библиотека для хранения данных",
      "Фреймворк для масштабируемого обучения и инференса больших моделей",
      "Язык программирования",
      "Инструмент для разметки данных"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 121,
    "question": "Что подразумевается под 4D Parallelism в контексте машинного обучения?",
    "type": "single",
    "options": [
      "Использование четырех GPU",
      "Комбинация data, tensor, pipeline и sequence parallelism",
      "Работа только с 4D тензорами",
      "Четырехшаговое обучение"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 122,
    "question": "Что такое Parameter-Efficient Fine-Tuning (PEFT)?",
    "type": "single",
    "options": [
      "Полный fine-tuning всех параметров модели",
      "Адаптация модели с минимальным числом обучаемых параметров",
      "Обучение без градиентов",
      "Квантование модели"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 123,
    "question": "Что такое Adapter Tuning в контексте обучения моделей?",
    "type": "single",
    "options": [
      "Замена весов модели",
      "Добавление небольших обучаемых модулей между слоями",
      "Удаление слоев модели",
      "Использование prompt-only обучения"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 124,
    "question": "Что такое Prefix Tuning?",
    "type": "single",
    "options": [
      "Обучение только embedding слоя",
      "Обучение добавленных обучаемых префиксных векторов",
      "Полный fine-tuning модели",
      "Удаление attention слоев"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 125,
    "question": "В чем заключается суть Prompt Tuning?",
    "type": "single",
    "options": [
      "Ручное написание промптов",
      "Обучение непрерывных prompt embedding при замороженной модели",
      "Обучение decoder слоев",
      "Использование reinforcement learning"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 126,
    "question": "Что такое Low-Rank Adaptation (LoRA)?",
    "type": "single",
    "options": [
      "Квантование весов",
      "Добавление низкоранговых матриц для адаптации весов",
      "Удаление attention",
      "Сжатие embeddings"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 127,
    "question": "Что такое Model Merging in PEFT?",
    "type": "single",
    "options": [
      "Обучение нескольких моделей с нуля",
      "Объединение адаптационных параметров в основную модель",
      "Удаление adapter слоев",
      "Слияние датасетов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 128,
    "question": "Что такое Reinforcement Learning from Human Feedback (RLHF)?",
    "type": "single",
    "options": [
      "Обучение без награды",
      "Обучение модели с использованием человеческих предпочтений как сигнала награды",
      "Классическое supervised обучение",
      "Онлайн обучение без данных"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 129,
    "question": "Какие компоненты входят в систему RLHF?",
    "type": "multiple",
    "options": [
      "Policy model",
      "Reward model",
      "Human preference data",
      "Optimizer state sharding"
    ],
    "correct": [
      0,
      1,
      2
    ]
  },
  {
    "id": 130,
    "question": "Что такое Retrieval-Augmented Generation (RAG)?",
    "type": "single",
    "options": [
      "Обучение модели с нуля",
      "Комбинация retrieval и генерации ответа моделью",
      "Метод сжатия модели",
      "Тип decoding стратегии"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 131,
    "question": "Что такое Approximate Nearest Neighbors (ANN)?",
    "type": "single",
    "options": [
      "Точный поиск ближайших векторов",
      "Приближенный, но более быстрый поиск ближайших векторов",
      "Поиск без индексов",
      "Поиск только по CPU"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 132,
    "question": "Что такое FAISS?",
    "type": "single",
    "options": [
      "Язык программирования",
      "Библиотека для эффективного поиска по векторам",
      "Dataset",
      "Модель эмбеддингов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 133,
    "question": "Что такое Tool Calling в контексте LLM агентов?",
    "type": "single",
    "options": [
      "Обучение модели",
      "Способ позволить модели вызывать внешние инструменты",
      "Метод retrieval",
      "Тип decoding"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 134,
    "question": "Что такое планирование (Planning) в контексте LLM агентов?",
    "type": "single",
    "options": [
      "Генерация токенов",
      "Разбиение задачи на последовательность шагов",
      "Обучение модели",
      "Выбор temperature"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 135,
    "question": "Что означает аббревиатура ReAct в контексте LLM агентов?",
    "type": "single",
    "options": [
      "Метод квантования",
      "Подход, совмещающий reasoning и действия",
      "Тип embedding",
      "Метод retrieval"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 136,
    "question": "Что означает рефлексия (reflection) в контексте LLM агентов?",
    "type": "single",
    "options": [
      "Повторная генерация без анализа",
      "Самоанализ и корректировка стратегии",
      "Снижение температуры",
      "Очистка памяти"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 137,
    "question": "Какова роль Vision Encoder в визуально-языковых моделях?",
    "type": "single",
    "options": [
      "Модуль генерации текста",
      "Модуль преобразования изображения в эмбеддинг",
      "Модуль генерации изображений",
      "Модуль retrieval"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 138,
    "question": "Что такое Classifier-Free Guidance в Stable Diffusion?",
    "type": "single",
    "options": [
      "Отказ от классификатора",
      "Управление генерацией без отдельного классификатора",
      "Использование supervised learning",
      "Метод retrieval"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 139,
    "question": "Что такое Stable Diffusion?",
    "type": "single",
    "options": [
      "GAN модель",
      "Latent Diffusion Model с text conditioning",
      "Модель классификации",
      "VLM"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 140,
    "question": "Что такое Visual Question Answering?",
    "type": "single",
    "options": [
      "Генерация изображений",
      "Ответы на вопросы по изображению",
      "Поиск изображений",
      "Сегментация"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 141,
    "question": "Из каких двух основных этапов состоит диффузионный процесс?",
    "type": "single",
    "options": [
      "Encoder и decoder",
      "Forward diffusion и reverse diffusion",
      "Training и inference",
      "Denoising и upscaling"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 142,
    "question": "Какую роль играет U-Net в архитектуре Stable Diffusion?",
    "type": "single",
    "options": [
      "Кодирование текста",
      "Предсказание шума на каждом шаге диффузии",
      "Генерация эмбеддингов изображений",
      "Классификация изображений"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 143,
    "question": "Что такое Vector Quantized VAE (VQ-VAE)?",
    "type": "single",
    "options": [
      "GAN архитектура",
      "Вариационный автоэнкодер с дискретным латентным пространством",
      "Автогрегрессионная модель",
      "Diffusion модель"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 144,
    "question": "Какую функцию выполняет VAE (Variational Autoencoder) в Stable Diffusion?",
    "type": "single",
    "options": [
      "Генерация текста",
      "Переход между пиксельным и латентным пространством",
      "Предсказание шума",
      "Управление attention"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 145,
    "question": "Что такое SDXL (Stable Diffusion XL)?",
    "type": "single",
    "options": [
      "GAN модель",
      "Улучшенная версия Stable Diffusion с более крупной архитектурой",
      "VLM модель",
      "Только upscaler"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 146,
    "question": "Что такое ControlNet для Stable Diffusion?",
    "type": "single",
    "options": [
      "Модель классификации изображений",
      "Механизм точного контроля генерации по структурным условиям",
      "Метод квантования",
      "Decoder текста"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 147,
    "question": "Для чего используются negative prompts в Stable Diffusion?",
    "type": "single",
    "options": [
      "Ускорение генерации",
      "Подавление нежелательных признаков в изображении",
      "Увеличение разрешения",
      "Снижение шума входных данных"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 148,
    "question": "Зачем нужен timestep embedding в U-Net диффузионной модели?",
    "type": "single",
    "options": [
      "Кодирование текста",
      "Информирование модели о текущем шаге диффузии",
      "Сжатие изображений",
      "Обучение attention"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 149,
    "question": "Что позволяет делать режим img2img в Stable Diffusion?",
    "type": "single",
    "options": [
      "Генерировать изображения только из текста",
      "Модифицировать существующее изображение с учетом промпта",
      "Обучать модель",
      "Классифицировать изображения"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 150,
    "question": "Почему Stable Diffusion работает в латентном пространстве, а не в пиксельном?",
    "type": "single",
    "options": [
      "Для повышения качества изображений",
      "Для снижения вычислительной сложности",
      "Для работы без VAE",
      "Для упрощения обучения text encoder"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 151,
    "question": "Что такое Image Captioning в контексте VLM?",
    "type": "single",
    "options": [
      "Поиск изображений",
      "Генерация текстового описания изображения",
      "Сегментация изображений",
      "Text-to-image генерация"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 152,
    "question": "Что такое Q-Former в архитектуре BLIP-2?",
    "type": "single",
    "options": [
      "Vision encoder",
      "Модуль, преобразующий визуальные признаки в форму, совместимую с LLM",
      "Text decoder",
      "Diffusion sampler"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 153,
    "question": "Какие типы attention используются в типичной VLM?",
    "type": "multiple",
    "options": [
      "Self-attention",
      "Cross-attention",
      "Sparse attention",
      "Convolutional attention"
    ],
    "correct": [
      0,
      1
    ]
  },
  {
    "id": 154,
    "question": "Для чего используется cross-attention механизм в визуально-языковых моделях?",
    "type": "single",
    "options": [
      "Только для обработки изображений",
      "Для связывания визуальных и текстовых представлений",
      "Для квантования весов",
      "Для ускорения инференса"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 155,
    "question": "Какова роль vision encoder в визуально-языковых моделях?",
    "type": "single",
    "options": [
      "Генерация текста",
      "Преобразование изображения в эмбеддинги",
      "Ранжирование документов",
      "Управление decoding"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 156,
    "question": "Что означает zero-shot способность визуально-языковой модели?",
    "type": "single",
    "options": [
      "Обучение без данных",
      "Выполнение новых задач без дополнительного обучения",
      "Работу без текста",
      "Отказ от encoder"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 157,
    "question": "Зачем нужен проекционный слой между vision encoder и language model?",
    "type": "single",
    "options": [
      "Снижение количества параметров",
      "Приведение визуальных эмбеддингов к размерности LLM",
      "Генерация изображений",
      "Обучение tokenizer"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 158,
    "question": "Какие метрики используются для оценки retrieval в системах поиска?",
    "type": "multiple",
    "options": [
      "Recall@k",
      "Precision@k",
      "MRR",
      "BLEU"
    ],
    "correct": [
      0,
      1,
      2
    ]
  },
  {
    "id": 159,
    "question": "Что такое Hierarchical Navigable Small World (HNSW)?",
    "type": "single",
    "options": [
      "Loss функция",
      "Графовая структура для ANN поиска",
      "Тип embedding",
      "Метод кластеризации"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 160,
    "question": "Что такое Locality Sensitive Hashing (LSH)?",
    "type": "single",
    "options": [
      "Точный поиск ближайших соседей",
      "Метод приближенного поиска на основе хеширования",
      "Метод обучения эмбеддингов",
      "Тип distance функции"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 161,
    "question": "Что такое ANNOY?",
    "type": "single",
    "options": [
      "Dataset",
      "Библиотека для приближенного поиска ближайших соседей",
      "Loss функция",
      "LLM модель"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 162,
    "question": "Что такое Semantic Search?",
    "type": "single",
    "options": [
      "Поиск по ключевым словам",
      "Поиск по смысловым векторным представлениям",
      "Поиск без эмбеддингов",
      "Поиск по регулярным выражениям"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 163,
    "question": "Что такое Contrastive Loss?",
    "type": "single",
    "options": [
      "Loss для классификации",
      "Функция потерь для сближения похожих и разделения непохожих примеров",
      "Loss для генерации текста",
      "Регуляризация весов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 164,
    "question": "Что такое Triplet Loss?",
    "type": "single",
    "options": [
      "Loss для регрессии",
      "Функция потерь на основе anchor, positive и negative примеров",
      "Loss для генерации изображений",
      "Метод оптимизации"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 165,
    "question": "Что такое InfoNCE?",
    "type": "single",
    "options": [
      "Метод токенизации",
      "Контрастивная функция потерь",
      "Тип optimizer",
      "Архитектура модели"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 166,
    "question": "Что такое LangChain?",
    "type": "single",
    "options": [
      "LLM модель",
      "Фреймворк для построения LLM-приложений",
      "Dataset",
      "Vector DB"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 167,
    "question": "Что такое multi-agent система?",
    "type": "single",
    "options": [
      "Одна LLM",
      "Система из нескольких взаимодействующих агентов",
      "Pipeline обработки данных",
      "Retriever"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 168,
    "question": "В чем основное отличие пайплайна от агента?",
    "type": "single",
    "options": [
      "Пайплайн обучается, агент — нет",
      "Пайплайн выполняет фиксированную последовательность шагов, агент принимает решения на основе состояния",
      "Агент не использует LLM",
      "Пайплайн работает только офлайн"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 169,
    "question": "Какие типы памяти используются в LLM агентах?",
    "type": "multiple",
    "options": [
      "Краткосрочная (context / working memory)",
      "Долгосрочная (vector / episodic memory)",
      "Регистровая память",
      "Параметрическая память модели"
    ],
    "correct": [
      0,
      1,
      3
    ]
  },
  {
    "id": 170,
    "question": "Что такое Agents and Multi-Agent Systems?",
    "type": "single",
    "options": [
      "Метод оптимизации LLM",
      "Системы из автономных сущностей, взаимодействующих для достижения целей",
      "Тип tokenizer",
      "Архитектура трансформера"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 171,
    "question": "Что такое Lifted Structured Loss?",
    "type": "single",
    "options": [
      "Loss для классификации",
      "Контрастивная функция потерь для metric learning с учетом всех пар в батче",
      "Loss для language modeling",
      "Регуляризация attention"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 172,
    "question": "Что такое N-Pair Loss?",
    "type": "single",
    "options": [
      "Loss для бинарной классификации",
      "Обобщение triplet loss на несколько negative примеров",
      "Loss для генерации изображений",
      "Метод оптимизации"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 173,
    "question": "Что такое TensorRT-LLM?",
    "type": "single",
    "options": [
      "Фреймворк обучения LLM",
      "Библиотека NVIDIA для оптимизированного инференса LLM",
      "Vector database",
      "Optimizer"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 174,
    "question": "Что такое vLLM?",
    "type": "single",
    "options": [
      "Dataset для обучения LLM",
      "Inference engine с эффективным управлением KV-cache",
      "Optimizer для LLM",
      "Тип tokenizer"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 175,
    "question": "Что такое ONNX?",
    "type": "single",
    "options": [
      "Язык программирования",
      "Формат представления моделей для совместимости между фреймворками",
      "Тип optimizer",
      "Vector database"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 176,
    "question": "Что такое GGUF?",
    "type": "single",
    "options": [
      "Формат датасета",
      "Формат хранения и распространения LLM, используемый в llama.cpp",
      "Optimizer",
      "Loss функция"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 177,
    "question": "Что такое Ollama?",
    "type": "single",
    "options": [
      "Dataset для RLHF",
      "Инструмент для локального запуска и управления LLM",
      "Vector DB",
      "LLM архитектура"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 178,
    "question": "Что такое HuggingFaceTGI?",
    "type": "single",
    "options": [
      "Tokenizer",
      "Inference сервер для LLM",
      "Dataset",
      "Optimizer"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 179,
    "question": "Что такое OpenWeb UI?",
    "type": "single",
    "options": [
      "LLM модель",
      "Веб-интерфейс для взаимодействия с LLM",
      "Vector database",
      "Inference engine"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 180,
    "question": "Что такое sglang (LMSYS)?",
    "type": "single",
    "options": [
      "Dataset",
      "Язык/фреймворк для программирования LLM inference",
      "Optimizer",
      "Tokenizer"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 181,
    "question": "Что такое Sentence Transformers?",
    "type": "single",
    "options": [
      "Архитектура трансформера",
      "Библиотека для получения sentence embeddings",
      "Tokenizer",
      "Vector database"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 182,
    "question": "Что такое Reparametrization Trick в VAE?",
    "type": "single",
    "options": [
      "Метод оптимизации",
      "Трюк для обеспечения дифференцируемости при сэмплировании латентных переменных",
      "Тип loss функции",
      "Метод регуляризации"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 183,
    "question": "Что такое Variational Autoencoders (VAE)?",
    "type": "single",
    "options": [
      "GAN модель",
      "Генеративная модель с вероятностным латентным пространством",
      "Только encoder",
      "Только decoder"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 184,
    "question": "Какое ключевое преимущество диффузионных моделей перед GAN?",
    "type": "single",
    "options": [
      "Меньшее число параметров",
      "Более стабильное обучение",
      "Отсутствие необходимости в данных",
      "Быстрый inference"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 185,
    "question": "Какие основные компоненты входят в Retrieval-Augmented Generation (RAG)?",
    "type": "multiple",
    "options": [
      "Retriever",
      "Generator (LLM)",
      "Reward model",
      "Index / Vector store"
    ],
    "correct": [
      0,
      1,
      3
    ]
  },
  {
    "id": 186,
    "question": "Что такое Image-Text Masking (ITM)?",
    "type": "single",
    "options": [
      "Метод data augmentation",
      "Задача определения соответствия изображения и текста",
      "Тип attention",
      "Метод генерации изображений"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 187,
    "question": "Какая задача решается с помощью модели Okapi BM25?",
    "type": "single",
    "options": [
      "Dense retrieval",
      "Sparse retrieval на основе TF-IDF",
      "Генерация текста",
      "Кластеризация документов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 188,
    "question": "Что такое Locality Sensitive Hashing (LSH)?",
    "type": "single",
    "options": [
      "Точный поиск ближайших соседей",
      "Метод приближенного поиска, сохраняющий близость объектов при хешировании",
      "Loss функция",
      "Тип embedding"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 189,
    "question": "Что означает conditioning в контексте Stable Diffusion?",
    "type": "single",
    "options": [
      "Обучение модели",
      "Условное управление генерацией с помощью текста или других сигналов",
      "Оптимизация весов",
      "Уменьшение шума"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 190,
    "question": "В чем заключается задача text-to-image retrieval?",
    "type": "single",
    "options": [
      "Генерация изображений по тексту",
      "Поиск изображений по текстовому запросу",
      "Генерация текста по изображению",
      "Классификация изображений"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 191,
    "question": "Для чего используется seed в Stable Diffusion?",
    "type": "single",
    "options": [
      "Управление learning rate",
      "Воспроизводимость генерации изображений",
      "Выбор text encoder",
      "Ускорение инференса"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 192,
    "question": "Зачем нужно позиционное кодирование в Vision Transformer?",
    "type": "single",
    "options": [
      "Для нормализации пикселей",
      "Для сохранения информации о пространственном расположении патчей",
      "Для уменьшения числа параметров",
      "Для ускорения обучения"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 193,
    "question": "Как Accelerate Semantic Search помогает в поиске?",
    "type": "single",
    "options": [
      "Использует ключевые слова вместо эмбеддингов",
      "Ускоряет векторный поиск с помощью оптимизированных индексов и ANN",
      "Удаляет необходимость в retriever",
      "Работает только с TF-IDF"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 194,
    "question": "Как Vision Transformer (ViT) обрабатывает изображения?",
    "type": "single",
    "options": [
      "Использует свёртки разных размеров",
      "Разбивает изображение на патчи и обрабатывает их как последовательность",
      "Преобразует изображение в граф",
      "Использует рекуррентные слои"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 195,
    "question": "Как работает обучение модели CLIP?",
    "type": "single",
    "options": [
      "Через автоэнкодер",
      "Через контрастивное обучение на парах изображение–текст",
      "Через RLHF",
      "Через masked language modeling"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 196,
    "question": "Как работают Attention Mechanisms in Multimodal Systems?",
    "type": "single",
    "options": [
      "Обрабатывают только одну модальность",
      "Позволяют одной модальности фокусироваться на релевантных частях другой",
      "Используются только для ускорения инференса",
      "Заменяют encoder"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 197,
    "question": "Какая модель обычно используется как text encoder в Stable Diffusion?",
    "type": "single",
    "options": [
      "BERT",
      "CLIP Text Encoder",
      "GPT",
      "T5"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 198,
    "question": "Какие компоненты входят в архитектуру PaliGemma?",
    "type": "multiple",
    "options": [
      "Vision encoder",
      "Language model",
      "Projection / adapter слой",
      "Diffusion U-Net"
    ],
    "correct": [
      0,
      1,
      2
    ]
  },
  {
    "id": 199,
    "question": "Какой подход часто используется для эффективного файн-тюнинга больших VLM?",
    "type": "single",
    "options": [
      "Обучение с нуля",
      "Parameter-Efficient Fine-Tuning (LoRA, adapters)",
      "Полное размораживание всех слоев",
      "Reinforcement learning"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 200,
    "question": "Чем DDIM sampler отличается от DDPM?",
    "type": "single",
    "options": [
      "Использует GAN",
      "Позволяет детерминированный и более быстрый sampling",
      "Не использует U-Net",
      "Работает без шума"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 201,
    "question": "Что включает в себя LLM Evaluation?",
    "type": "multiple",
    "options": [
      "Автоматические метрики",
      "Human evaluation",
      "Benchmark-тесты",
      "Обучение модели"
    ],
    "correct": [
      0,
      1,
      2
    ]
  },
  {
    "id": 202,
    "question": "Что контролирует параметр CFG (Classifier-Free Guidance) scale?",
    "type": "single",
    "options": [
      "Размер латентного пространства",
      "Баланс между следованием условию и разнообразием генерации",
      "Количество denoising steps",
      "Learning rate"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 203,
    "question": "Что представляет собой ToolMessage в LangChain?",
    "type": "single",
    "options": [
      "Ответ LLM",
      "Сообщение с результатом выполнения инструмента",
      "System prompt",
      "Embedding"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 204,
    "question": "Что такое Alignment Data Collection в контексте RLHF?",
    "type": "single",
    "options": [
      "Сбор неразмеченных текстов",
      "Сбор человеческих предпочтений и оценок ответов модели",
      "Автоматическая генерация данных",
      "Inference модели"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 205,
    "question": "Что такое CLIP model?",
    "type": "single",
    "options": [
      "Diffusion модель",
      "Мультимодальная модель, обученная на парах изображение–текст",
      "GAN архитектура",
      "Только text encoder"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 206,
    "question": "Что такое Cosine Similarity?",
    "type": "single",
    "options": [
      "Евклидово расстояние",
      "Мера угла между векторами",
      "Разница длин векторов",
      "Количество совпадающих координат"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 207,
    "question": "Что такое Diffusion Models for Multimodal Generation?",
    "type": "single",
    "options": [
      "Модели классификации",
      "Диффузионные модели, использующие несколько модальностей как условия",
      "GAN модели",
      "Только текстовые модели"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 208,
    "question": "Что такое Dot Product?",
    "type": "single",
    "options": [
      "Расстояние между векторами",
      "Скалярное произведение векторов",
      "Нормализация векторов",
      "Тип loss функции"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 209,
    "question": "Что такое Euclidean Distance?",
    "type": "single",
    "options": [
      "Косинусное сходство",
      "Стандартное расстояние между точками в пространстве",
      "Скалярное произведение",
      "Метод оптимизации"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 210,
    "question": "Что такое Generative Adversarial Networks (GANs) for Multimodal Generation?",
    "type": "single",
    "options": [
      "Классификационные модели",
      "GAN, генерирующие данные с учетом нескольких модальностей",
      "Diffusion модели",
      "Retrieval системы"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 211,
    "question": "Что такое Positional Encodings in Multimodal Language Models?",
    "type": "single",
    "options": [
      "Embedding слов",
      "Кодирование порядка и положения элементов разных модальностей",
      "Метод квантования",
      "Тип attention"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 212,
    "question": "Что такое Prompt Engineering?",
    "type": "single",
    "options": [
      "Обучение модели",
      "Проектирование входных запросов для управления поведением модели",
      "Оптимизация весов",
      "Квантование модели"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 213,
    "question": "Что такое Vision Transformer (ViT)?",
    "type": "single",
    "options": [
      "CNN архитектура",
      "Transformer, применяемый к изображениям через патчи",
      "GAN модель",
      "Diffusion модель"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 214,
    "question": "Что такое LoRA в контексте Stable Diffusion?",
    "type": "single",
    "options": [
      "Optimizer",
      "Метод параметро-эффективного файн-тюнинга",
      "Sampler",
      "Text encoder"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 215,
    "question": "Что такое denoising steps в Stable Diffusion?",
    "type": "single",
    "options": [
      "Шаги обучения",
      "Количество итераций удаления шума при генерации",
      "Размер батча",
      "CFG scale"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 216,
    "question": "Что такое few-shot learning для визуально-языковых моделей?",
    "type": "single",
    "options": [
      "Обучение без данных",
      "Адаптация модели с небольшим числом примеров",
      "Обучение только vision encoder",
      "Inference без промптов"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 217,
    "question": "Что такое fusion strategies в мультимодальных моделях?",
    "type": "single",
    "options": [
      "Методы объединения информации из разных модальностей",
      "Loss функции",
      "Тип tokenizer",
      "Метод оптимизации"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 218,
    "question": "Что такое image-to-text retrieval?",
    "type": "single",
    "options": [
      "Генерация текста по изображению",
      "Поиск текста по изображению",
      "Классификация изображений",
      "Text-to-image генерация"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 219,
    "question": "Что такое prompt engineering для Stable Diffusion?",
    "type": "single",
    "options": [
      "Файн-тюнинг модели",
      "Создание текстовых промптов для управления генерацией изображений",
      "Изменение U-Net",
      "Настройка optimizer"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 220,
    "question": "Что такое scheduler в контексте диффузионных моделей?",
    "type": "single",
    "options": [
      "Optimizer",
      "Алгоритм управления шагами шума и denoising",
      "Text encoder",
      "Loss функция"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 221,
    "question": "Что такое textual inversion embeddings?",
    "type": "single",
    "options": [
      "Метод сжатия текста",
      "Обучаемые текстовые эмбеддинги для представления новых концептов",
      "Тип tokenizer",
      "Метод sampling"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 222,
    "question": "Что такое модальность в контексте мультимодальных моделей?",
    "type": "single",
    "options": [
      "Тип optimizer",
      "Тип данных (текст, изображение, аудио и т.д.)",
      "Архитектура модели",
      "Loss функция"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 223,
    "question": "Какая команда победит в НТО по профилю 'Геопространственные цифровые двойники'?",
    "type": "single",
    "options": [
      "Здесь могла быть ваша реклама",
      "Ленивые тюлени",
      "Команда без названия",
      "Название без команды"
    ],
    "correct": [
      1
    ]
  },
  {
    "id": 224,
    "question": "你好，玛莎！你都好吗？",
    "type": "multiple",
    "options": [
      "很好",
      "太好了"
    ],
    "correct": [
      0, 1
    ]
  },
  {
    "id": 225,
    "question": "你叫什么名字？",
    "type": "single",
    "options": [
      "热尼亚",
      "玛莎",
      "你好",
      "莫斯科"
    ],
    "correct": [
      1
    ]
  }
]