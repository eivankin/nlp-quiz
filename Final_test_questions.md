### Вопрос: Память при обучении с Momentum (FP32)
**Вопрос:** Сколько памяти потребуется для обучения 7B модели в формате **FP32** с использованием оптимизатора **Momentum**, если сама модель требует 28 ГБ для хранения параметров?

---

### Вопрос: Память при обучении с RMSProp (4-bit)
**Вопрос:** Сколько памяти потребуется для обучения 7B модели в формате **4-bit** с использованием оптимизатора **RMSProp**, если параметры занимают 3.5 ГБ?


---

### Вопрос: Память при обучении с Adam (FP16)
**Вопрос:** Сколько памяти потребуется для хранения параметров, градиентов, экспоненциального среднего градиентов (m) и среднего квадратов градиентов (v) для 7B модели в **FP16** с оптимизатором **Adam**?


---

### Вопрос: Разница в потреблении памяти при инференсе и обучении (8-bit)
**Вопрос:** Какое основное отличие в потреблении памяти для 7B модели в формате **8-bit** при **инференсе** по сравнению с **обучением** с оптимизатором **Adagrad**?


---

### Вопрос: Работа с библиотекой Transformers  
**Вопрос:** Какая функция из библиотеки **Transformers** используется для загрузки предобученной модели?  


---

### Вопрос: Типы токенизации в Tokenizers  
**Вопрос:** Какой тип токенизации поддерживается библиотекой **Tokenizers**, чтобы эффективно работать с низкоресурсными языками?  


---

### Вопрос: Использование PEFT для эффективного обучения  
**Вопрос:** Какой метод из библиотеки **PEFT** используется для адаптации модели с помощью параметров низкого ранга?  


---

### Вопрос: TRL и RLHF  
**Вопрос:** Какой метод библиотеки **TRL** используется для реализации обучения с подкреплением от человеческой обратной связи (RLHF)?  


---

### Вопрос: Sentence Transformers и задачи эмбеддингов  
**Вопрос:** Какая функция в библиотеке **Sentence Transformers** используется для вычисления эмбеддингов текста?  


---

### Вопрос: Регулярные выражения в Python  
**Вопрос:** Какая функция модуля `re` используется для поиска всех непересекающихся совпадений шаблона в строке?  


---

### Вопрос: Нормализация текста  
**Вопрос:** Какой метод нормализации используется для преобразования текста в нижний регистр?  


---

### Вопрос: Стемминг  
**Вопрос:** Какая библиотека Python чаще всего используется для выполнения стемминга?  


---

### Вопрос: Типы токенизации  
**Вопрос:** Какой из приведённых типов токенизации основан на разбиении текста на предложения?  


---

### Вопрос: Byte-Pair Encoding (BPE)  
**Вопрос:** Какой шаг выполняется в алгоритме Byte-Pair Encoding (BPE)?  


---

### Вопрос: SentencePiece и субсловная токенизация  
**Вопрос:** Какой метод токенизации реализован в библиотеке SentencePiece?  


---

### Вопрос: WordPiece  
**Вопрос:** Какой принцип лежит в основе алгоритма **WordPiece**?  


---

### Вопрос: One-Hot Encoding  
**Вопрос:** Какая из следующих характеристик описывает **One-Hot Encoding**?  


---

### Вопрос: Bag of Words  
**Вопрос:** Какую информацию сохраняет модель **Bag of Words**?  


---

### Вопрос: Naive Bayes  
**Вопрос:** Какая из следующих задач может быть решена с помощью **Naive Bayes**?  


---

### Вопрос: TF-IDF  
**Вопрос:** Что означает компонент **IDF** в термине **TF-IDF**?  


---

### Вопрос: Okapi BM25  
**Вопрос:** Какая задача решается с помощью модели **Okapi BM25**?  


---

### Вопрос: Cosine Similarity  
**Вопрос:** Что измеряет **Cosine Similarity**?  


---

### Вопрос: N-gram Models  
**Вопрос:** Какая основная задача решается с помощью **n-граммных моделей**?  


---

### Вопрос: Cross-Entropy  
**Вопрос:** Что измеряет функция потерь **Cross-Entropy**?  


---

### Вопрос: Perplexity  
**Вопрос:** Что характеризует **Perplexity** в языковых моделях?  


---

### Вопрос: BLEU Score  
**Вопрос:** В каких задачах используется **BLEU Score**?  


---

### Вопрос: ROUGE Score  
**Вопрос:** Для какой задачи применяется **ROUGE Score**?  


---

### Вопрос: Precision  
**Вопрос:** Как интерпретируется метрика **Precision**?  


---

### Вопрос: Recall  
**Вопрос:** Что измеряет метрика **Recall**?  


---

### Вопрос: F1 Score  
**Вопрос:** Как определяется **F1 Score**?  


---

### Вопрос: Cross-Entropy Loss  
**Вопрос:** В чем основное назначение функции потерь **Cross-Entropy Loss**?  


---

### Вопрос: Backpropagation in Neural Networks  
**Вопрос:** Какова основная цель **Backpropagation** в нейронных сетях?  


---

### Вопрос: Backpropagation and the Chain Rule  
**Вопрос:** Как используется **Chain Rule** в алгоритме Backpropagation?  


---

### Вопрос: Popular Optimization Methods in Deep Learning  
**Вопрос:** Какой из следующих методов оптимизации используется в глубоком обучении?  


---

### Вопрос: Adam (Adaptive Moment Estimation)  
**Вопрос:** Чем **Adam** отличается от стандартного градиентного спуска?  


---

### Вопрос: Activation Functions  
**Вопрос:** Какую функцию активации предпочтительно использовать для нейронов скрытых слоев?  


---

### Вопрос: Loss Function  
**Вопрос:** Какую функцию потерь обычно используют для задач классификации?  


---

### Вопрос: CNN in NLP  
**Вопрос:** Какое преимущество дают сверточные нейронные сети (CNN) в задачах обработки естественного языка (NLP)?  


---

### Вопрос: Word2Vec  
**Вопрос:** Какая идея лежит в основе **Word2Vec**?  


---

### Вопрос: GloVe Model  
**Вопрос:** Чем **GloVe** отличается от **Word2Vec**?  


---

### Вопрос: FastText Model  
**Вопрос:** Какое ключевое отличие модели **FastText** от **Word2Vec**?  


---

### Вопрос: Vanilla RNN  
**Вопрос:** Какова основная проблема **Vanilla RNN**?  


---

### Вопрос: LSTM  
**Вопрос:** Чем **LSTM** (Long Short-Term Memory) отличается от стандартных RNN?  


---

### Вопрос: Bidirectional LSTM  
**Вопрос:** Какую дополнительную информацию дает **Bidirectional LSTM**?  


---

### Вопрос: GRU  
**Вопрос:** Чем **GRU** (Gated Recurrent Unit) отличается от **LSTM**?  


---

### Вопрос: ELMo (Embeddings from Language Models)  
**Вопрос:** Какую основную идею реализует модель **ELMo**?  


---

### Вопрос: RNN-based Sequence-to-Sequence (Seq2Seq) Model  
**Вопрос:** Какова основная цель **RNN-based Sequence-to-Sequence (Seq2Seq) Model**?  


---

### Вопрос: Attention Mechanism  
**Вопрос:** Зачем используется **Attention Mechanism** в Seq2Seq моделях?  


---

### Вопрос: Bahdanau Attention  
**Вопрос:** Что является ключевой особенностью **Bahdanau Attention**?  


---

### Вопрос: Luong Attention  
**Вопрос:** Чем **Luong Attention** отличается от **Bahdanau Attention**?  


---

### Вопрос: RNN-based Seq2Seq Model with Attention  
**Вопрос:** Какое преимущество добавляет **Attention** к стандартной Seq2Seq модели?  


---

### Вопрос: Transformer Encoder-Decoder Architecture  
**Вопрос:** Какое отличие **Transformer Encoder-Decoder Architecture** от Seq2Seq на основе RNN?  


---

### Вопрос: Transformer Encoder Architecture  
**Вопрос:** Какую задачу решает **Transformer Encoder**?  


---

### Вопрос: Transformer Decoder Architecture  
**Вопрос:** Какую роль выполняет **Transformer Decoder**?  


---

### Вопрос: Self-Attention  
**Вопрос:** Какова основная цель механизма **Self-Attention**?  


---

### Вопрос: Cross-Attention  
**Вопрос:** Чем отличается **Cross-Attention** от **Self-Attention**?  


---

### Вопрос: Multi-Head Attention  
**Вопрос:** Какую проблему решает **Multi-Head Attention**?  


---

### Вопрос: Sliding Window Attention  
**Вопрос:** Какова основная цель использования **Sliding Window Attention**?  


---

### Вопрос: Layer Normalization  
**Вопрос:** Для чего используется **Layer Normalization** в трансформерах?  


---

### Вопрос: Residual Connections  
**Вопрос:** Какую функцию выполняют **Residual Connections** в трансформерах?  


---

### Вопрос: Masked Language Modeling  
**Вопрос:** Какова основная цель задачи **Masked Language Modeling (MLM)**?  


---

### Вопрос: Adding a Custom Classification Head to a Transformer  
**Вопрос:** Какой компонент обычно добавляется к трансформеру для выполнения классификации?  


---

### Вопрос: Sampling Strategies  
**Вопрос:** Какую роль играют **Sampling Strategies** при генерации текста?  


---

### Вопрос: Greedy Sampling  
**Вопрос:** Как работает **Greedy Sampling**?  


---

### Вопрос: Stochastic Sampling  
**Вопрос:** Чем **Stochastic Sampling** отличается от **Greedy Sampling**?  


---

### Вопрос: Top-k Sampling  
**Вопрос:** Какова основная идея **Top-k Sampling**?  


---

### Вопрос: Top-p (Nucleus) Sampling  
**Вопрос:** Что контролирует параметр **p** в **Top-p Sampling**?  


---

### Вопрос: Temperature Scaling  
**Вопрос:** Как параметр **Temperature** влияет на процесс генерации текста?  


---

### Вопрос: Decoding Strategies  
**Вопрос:** Для чего используются **Decoding Strategies**?  


---

### Вопрос: Greedy Search  
**Вопрос:** Какой недостаток может возникнуть при использовании **Greedy Search**?  


---

### Вопрос: Beam Search  
**Вопрос:** Как **Beam Search** улучшает генерацию текста?  


---

### Вопрос: Beam-search Multinomial Sampling  
**Вопрос:** Какова основная цель использования **Beam-search Multinomial Sampling**?  


---

### Вопрос: Diverse Beam Search  
**Вопрос:** Как **Diverse Beam Search** отличается от стандартного **Beam Search**?  


---

### Вопрос: Speculative Decoding  
**Вопрос:** Как **Speculative Decoding** помогает ускорить генерацию текста?  


---

### Вопрос: Assisted Generation  
**Вопрос:** Что подразумевается под **Assisted Generation**?  


---

### Вопрос: Scaling Laws  
**Вопрос:** Какую зависимость описывают **Scaling Laws**?  


---

### Вопрос: In-context Learning  
**Вопрос:** Каковы ключевые особенности **In-context Learning**?  


---

### Вопрос: Instruction Following  
**Вопрос:** В чем состоит ключевая идея **Instruction Following**?  


---

### Вопрос: Step-by-Step Reasoning  
**Вопрос:** Почему **Step-by-Step Reasoning** важен для моделей NLP?  


---

### Вопрос: BERT  
**Вопрос:** Какова основная архитектурная особенность **BERT**?  


---

### Вопрос: ALBERT  
**Вопрос:** Какое улучшение предложил **ALBERT** по сравнению с **BERT**?  


---

### Вопрос: GPT  
**Вопрос:** Какова основная задача обучения **GPT**?  


---

### Вопрос: T5  
**Вопрос:** В чем особенность модели **T5**?  


---

### Вопрос: XLNet  
**Вопрос:** Как **XLNet** улучшает двунаправленное моделирование по сравнению с **BERT**?  


---

### Вопрос: Absolute Positional Encoding  
**Вопрос:** В чем заключается основная идея **Absolute Positional Encoding** в трансформерах?  


---

### Вопрос: Relative Positional Encodings  
**Вопрос:** Чем **Relative Positional Encodings** отличаются от **Absolute Positional Encoding**?  


---

### Вопрос: Rotary Position Embeddings  
**Вопрос:** Какая ключевая характеристика **Rotary Position Embeddings**?  


---

### Вопрос: Parameter Sharing  
**Вопрос:** Зачем используется **Parameter Sharing** в глубоких моделях?  


---

### Вопрос: Multi/Grouped Query Attention  
**Вопрос:** Что добавляет **Multi/Grouped Query Attention** по сравнению с обычным механизмом внимания?  


---

### Вопрос: KV Cache  
**Вопрос:** Как **KV Cache** помогает ускорить генерацию текста?  


---

### Вопрос: FlashAttention  
**Вопрос:** В чем ключевое преимущество **FlashAttention**?  


---

### Вопрос: PagedAttention  
**Вопрос:** Какова основная цель **PagedAttention**?  


---

### Вопрос: Mixture of Experts  
**Вопрос:** Как работает подход **Mixture of Experts**?  


---

### Вопрос: Knowledge Distillation  
**Вопрос:** Какова основная идея **Knowledge Distillation**?  


---

### Вопрос: Distributed Training  
**Вопрос:** Что подразумевается под **Distributed Training** в контексте глубокого обучения?  


---

### Вопрос: Data Parallelism  
**Вопрос:** Какова цель **Data Parallelism** в распределенном обучении?  


---

### Вопрос: Pipeline (Model) Parallelism  
**Вопрос:** В чем заключается принцип **Pipeline (Model) Parallelism**?  


---

### Вопрос: Tensor Parallelism  
**Вопрос:** Как работает **Tensor Parallelism**?  


---

### Вопрос: Mixed Precision Training  
**Вопрос:** Что такое **Mixed Precision Training**?  


---

### Вопрос: Gradient Accumulation and Synchronization  
**Вопрос:** Что такое **Gradient Accumulation and Synchronization**?  


---

### Вопрос: All-Reduce for Gradient Aggregation  
**Вопрос:** Для чего используется **All-Reduce for Gradient Aggregation** в распределенном обучении?  


---

### Вопрос: Quantization  
**Вопрос:** Что такое **Quantization** в контексте машинного обучения?  


---

### Вопрос: Post-Training Quantization  
**Вопрос:** Что такое **Post-Training Quantization**?  


---

### Вопрос: Quantization-Aware Training  
**Вопрос:** Как работает **Quantization-Aware Training**?  


---

### Вопрос: Distillation  
**Вопрос:** Что такое **Distillation** в контексте машинного обучения?  


---

### Вопрос: Logit-Based Distillation  
**Вопрос:** Что такое **Logit-Based Distillation**?  


---

### Вопрос: Feature-Based Distillation  
**Вопрос:** В чем заключается принцип **Feature-Based Distillation**?  


---

### Вопрос: Relational Distillation  
**Вопрос:** Что подразумевается под **Relational Distillation**?  


---

### Вопрос: Pruning  
**Вопрос:** Что такое **Pruning** в контексте нейронных сетей?  


---

### Вопрос: Corpora for Pre-training  
**Вопрос:** Что такое **Corpora for Pre-training**?  


---

### Вопрос: Instruction Tuning Datasets  
**Вопрос:** Что такое **Instruction Tuning Datasets**?  


---

### Вопрос: Alignment Datasets  
**Вопрос:** В чем заключается назначение **Alignment Datasets**?  


---

### Вопрос: Filtering and Selection  
**Вопрос:** Какую роль выполняет **Filtering and Selection** в подготовке данных для обучения?  


---

### Вопрос: De-duplication  
**Вопрос:** Что подразумевается под **De-duplication** в обработке данных?  


---

### Вопрос: Privacy Reduction  
**Вопрос:** Что означает **Privacy Reduction** в контексте обработки данных?  


---

### Вопрос: Data Scheduling for Pre-training LLMs  
**Вопрос:** Что такое **Data Scheduling for Pre-training LLMs**?  


---

### Вопрос: Data Mixture  
**Вопрос:** Что такое **Data Mixture** в контексте обучения больших моделей?  


---

### Вопрос: Data Curriculum  
**Вопрос:** Что такое **Data Curriculum** в обучении моделей?  


---

### Вопрос: DeepSpeed lib  
**Вопрос:** Что такое **DeepSpeed lib**?  


---

### Вопрос: Megatron-LM  
**Вопрос:** Что такое **Megatron-LM**?  


---

### Вопрос: Colossal-AI  
**Вопрос:** Что такое **Colossal-AI**?  


---

### Вопрос: 4D Parallelism  
**Вопрос:** Что подразумевается под **4D Parallelism** в контексте машинного обучения?  


---

### Вопрос: Parameter-Efficient Fine-Tuning  
**Вопрос:** Что такое **Parameter-Efficient Fine-Tuning** (PEFT)?  


---

### Вопрос: Adapter Tuning  
**Вопрос:** Что такое **Adapter Tuning** в контексте обучения моделей?  


---

### Вопрос: Prefix Tuning  
**Вопрос:** Что такое **Prefix Tuning**?  


---

### Вопрос: Prompt Tuning  
**Вопрос:** В чем заключается суть **Prompt Tuning**?  


---

### Вопрос: Low-Rank Adaptation (LoRA)  
**Вопрос:** Что такое **Low-Rank Adaptation (LoRA)**?  


---

### Вопрос: Model Merging in PEFT  
**Вопрос:** Что такое **Model Merging in PEFT**?  


---

### Вопрос: Reinforcement Learning from Human Feedback (RLHF)  
**Вопрос:** Что такое **Reinforcement Learning from Human Feedback (RLHF)**?  


---

### Вопрос: Components of the RLHF System  
**Вопрос:** Какие компоненты входят в систему **RLHF**?  


---

### Вопрос: Alignment Data Collection  
**Вопрос:** Что такое **Alignment Data Collection** в контексте RLHF?  


---

### Вопрос: LLM Evaluation  
**Вопрос:** Что включает в себя **LLM Evaluation**?  


---

### Вопрос: HuggingFaceTGI  
**Вопрос:** Что такое **HuggingFaceTGI**?  


---

### Вопрос: TensorRT-LLM  
**Вопрос:** Что такое **TensorRT-LLM**?  


---

### Вопрос: vllm  
**Вопрос:** Что такое **vllm**?  


---

### Вопрос: SGLang (LMSys) 
**Вопрос:** Что такое **SGLang (LMSys)**?  


---

### Вопрос: ONNX 
**Вопрос:** Что такое **ONNX**?  


---

### Вопрос: GGUF  
**Вопрос:** Что такое **GGUF**?  


---

### Вопрос: Ollama  
**Вопрос:** Что такое **Ollama**?  


---

### Вопрос: Prompt Engineering  
**Вопрос:** Что такое **Prompt Engineering**?  


---

### Вопрос: OpenWeb UI  
**Вопрос:** Что такое **OpenWeb UI**?  


---

### Вопрос: Semantic Search  
**Вопрос:** Что такое **Semantic Search**?  


---

### Вопрос: Cosine Similarity  
**Вопрос:** Что такое **Cosine Similarity**?  


---

### Вопрос: Euclidean Distance  
**Вопрос:** Что такое **Euclidean Distance**?  


---

### Вопрос: Dot Product  
**Вопрос:** Что такое **Dot Product**?  


---

### Вопрос: Approximate Nearest Neighbors (ANN)  
**Вопрос:** Что такое **Approximate Nearest Neighbors (ANN)**?  


---

### Вопрос: Hierarchical Navigable Small World (HNSW)  
**Вопрос:** Что такое **Hierarchical Navigable Small World (HNSW)**?  


---

### Вопрос: Locality Sensitive Hashing (LSH)  
**Вопрос:** Что такое **Locality Sensitive Hashing (LSH)**?  


---

### Вопрос: FAISS  
**Вопрос:** Что такое **FAISS**?  


---

### Вопрос: Annoy  
**Вопрос:** Что такое **Annoy**?  


---

### Вопрос: Accelerate Semantic Search  
**Вопрос:** Как **Accelerate Semantic Search** помогает в поиске?  


---

### Вопрос: Retrieval-Augmented Generation (RAG)  
**Вопрос:** Что такое **Retrieval-Augmented Generation (RAG)**?  


---

### Вопрос: Key Components of RAG  
**Вопрос:** Какие основные компоненты входят в **Retrieval-Augmented Generation (RAG)**?  


---

### Вопрос: Retrieval Metrics  
**Вопрос:** Какие метрики используются для оценки **retrieval** в системах поиска?  


---

### Вопрос: LangChain  
**Вопрос:** Что такое **LangChain**?  


---

### Вопрос: Sentence Transformers  
**Вопрос:** Что такое **Sentence Transformers**?  


---

### Вопрос: Agents and Multi-Agent Systems  
**Вопрос:** Что такое **Agents and Multi-Agent Systems**?  


---

### Вопрос: Attention Mechanisms in Multimodal Systems  
**Вопрос:** Как работают **Attention Mechanisms in Multimodal Systems**?  


---

### Вопрос: Positional Encodings in Multimodal Language Models  
**Вопрос:** Что такое **Positional Encodings in Multimodal Language Models**?  


---

### Вопрос: Contrastive Loss  
**Вопрос:** Что такое **Contrastive Loss**?  


---

### Вопрос: Triplet Loss  
**Вопрос:** Что такое **Triplet Loss**?  


---

### Вопрос: Lifted Structured Loss  
**Вопрос:** Что такое **Lifted Structured Loss**?  


---

### Вопрос: N-pair Loss  
**Вопрос:** Что такое **N-pair Loss**?  


---

### Вопрос: InfoNCE  
**Вопрос:** Что такое **InfoNCE**?  


---

### Вопрос: Image-Text Masking (ITM)  
**Вопрос:** Что такое **Image-Text Masking (ITM)**?  


---

### Вопрос: Variational Autoencoders  
**Вопрос:** Что такое **Variational Autoencoders (VAE)**?  


---

### Вопрос: Generative Adversarial Networks (GANs) for Multimodal Generation  
**Вопрос:** Что такое **Generative Adversarial Networks (GANs) for Multimodal Generation**?  


---

### Вопрос: Diffusion Models for Multimodal Generation  
**Вопрос:** Что такое **Diffusion Models for Multimodal Generation**?  


---

### Вопрос: Stable Diffusion  
**Вопрос:** Что такое **Stable Diffusion**?  


---

### Вопрос: Reparametrization Trick in VAE  
**Вопрос:** Что такое **Reparametrization Trick** в **VAE**?  


---

### Вопрос: Vector Quantized VAE (VQ-VAE)  
**Вопрос:** Что такое **Vector Quantized VAE (VQ-VAE)**?  


---

### Вопрос: Vision Transformer  
**Вопрос:** Что такое **Vision Transformer (ViT)**?  


---

### Вопрос: CLIP model  
**Вопрос:** Что такое **CLIP model**?  


---

### Вопрос: Structure of a Typical Vision-Language Model
Вопрос: Какова основная цель типичной структуры Vision-Language модели?


---

### Вопрос: Архитектура ReAct
**Вопрос:** Что означает аббревиатура ReAct в контексте LLM агентов?


---

### Вопрос: Tool Calling в агентах
**Вопрос:** Что такое Tool Calling в контексте LLM агентов?


---

### Вопрос: Память агента
**Вопрос:** Какие типы памяти используются в LLM агентах?


---

### Вопрос: Пайплайны vs Агенты
**Вопрос:** В чем основное отличие пайплайна от агента?


---

### Вопрос: Планирование в агентах
**Вопрос:** Что такое планирование (Planning) в контексте LLM агентов?


---

### Вопрос: ToolMessage в LangChain
**Вопрос:** Что представляет собой ToolMessage в LangChain?


---

### Вопрос: Vision Encoder в VLM
**Вопрос:** Какова роль Vision Encoder в визуально-языковых моделях?


---

### Вопрос: Проекционный слой в VLM
**Вопрос:** Зачем нужен проекционный слой между Vision Encoder и Language Model?


---

### Вопрос: CLIP архитектура
**Вопрос:** Как работает обучение модели CLIP?


---

### Вопрос: PaliGemma модель
**Вопрос:** Какие компоненты входят в архитектуру PaliGemma?


---

### Вопрос: Image Captioning
**Вопрос:** Что такое Image Captioning в контексте VLM?


---

### Вопрос: Visual Question Answering (VQA)
**Вопрос:** Что такое Visual Question Answering?


---

### Вопрос: Патчи в Vision Transformer
**Вопрос:** Как Vision Transformer (ViT) обрабатывает изображения?


---

### Вопрос: Позиционное кодирование в ViT
**Вопрос:** Зачем нужно позиционное кодирование в Vision Transformer?


---

### Вопрос: Cross-attention в VLM
**Вопрос:** Для чего используется cross-attention механизм в визуально-языковых моделях?


---

### Вопрос: Fine-tuning VLM
**Вопрос:** Какой подход часто используется для эффективного файн-тюнинга больших VLM?


---

### Вопрос: BLIP-2 архитектура
**Вопрос:** Что такое Q-Former в архитектуре BLIP-2?


---

### Вопрос: Stable Diffusion основы
**Вопрос:** Что такое Stable Diffusion?


---

### Вопрос: Диффузионный процесс
**Вопрос:** Из каких двух основных этапов состоит диффузионный процесс?


---

### Вопрос: Latent Space в Stable Diffusion
**Вопрос:** Почему Stable Diffusion работает в латентном пространстве, а не в пиксельном?


---

### Вопрос: U-Net в Stable Diffusion
**Вопрос:** Какую роль играет U-Net в архитектуре Stable Diffusion?


---

### Вопрос: VAE в Stable Diffusion
**Вопрос:** Какую функцию выполняет VAE (Variational Autoencoder) в Stable Diffusion?


---

### Вопрос: Text Encoder в Stable Diffusion
**Вопрос:** Какая модель обычно используется как text encoder в Stable Diffusion?


---

### Вопрос: Conditioning в диффузионных моделях
**Вопрос:** Что означает conditioning в контексте Stable Diffusion?


---

### Вопрос: Denoising steps
**Вопрос:** Что такое denoising steps в Stable Diffusion?


---

### Вопрос: Scheduler в диффузионных моделях
**Вопрос:** Что такое scheduler в контексте диффузионных моделей?


---

### Вопрос: Classifier-free guidance
**Вопрос:** Что такое classifier-free guidance в Stable Diffusion?


---

### Вопрос: Negative prompts
**Вопрос:** Для чего используются negative prompts в Stable Diffusion?


---

### Вопрос: LoRA для Stable Diffusion
**Вопрос:** Что такое LoRA в контексте Stable Diffusion?


---

### Вопрос: Img2Img в Stable Diffusion
**Вопрос:** Что позволяет делать режим Img2Img в Stable Diffusion?


---

### Вопрос: ControlNet
**Вопрос:** Что такое ControlNet для Stable Diffusion?


---

### Вопрос: DDIM sampler
**Вопрос:** Чем DDIM sampler отличается от DDPM?


---

### Вопрос: Embedding в Stable Diffusion
**Вопрос:** Что такое textual inversion embeddings?


---

### Вопрос: GAN vs Diffusion
**Вопрос:** Какое ключевое преимущество диффузионных моделей перед GAN?


---

### Вопрос: SDXL
**Вопрос:** Что такое SDXL (Stable Diffusion XL)?


---

### Вопрос: Timestep embedding
**Вопрос:** Зачем нужен timestep embedding в U-Net диффузионной модели?


---

### Вопрос: Attention mechanisms в VLM
**Вопрос:** Какие типы attention используются в типичной VLM?


---

### Вопрос: Агент с рефлексией
**Вопрос:** Что означает рефлексия (reflection) в контексте LLM агентов?


---

### Вопрос: Multi-agent системы
**Вопрос:** Что такое multi-agent система?


---

### Вопрос: Zero-shot VLM
**Вопрос:** Что означает zero-shot способность визуально-языковой модели?


---

### Вопрос: Few-shot learning в VLM
**Вопрос:** Что такое few-shot learning для визуально-языковых моделей?


---

### Вопрос: Image-to-Text Retrieval
**Вопрос:** Что такое image-to-text retrieval?


---

### Вопрос: Text-to-Image Retrieval
**Вопрос:** В чем заключается задача text-to-image retrieval?


---

### Вопрос: Модальность
**Вопрос:** Что такое модальность в контексте мультимодальных моделей?


---

### Вопрос: Fusion strategies
**Вопрос:** Что такое fusion strategies в мультимодальных моделях?


---

### Вопрос: Prompt engineering для Stable Diffusion
**Вопрос:** Что такое prompt engineering для Stable Diffusion?


---

### Вопрос: CFG Scale
**Вопрос:** Что контролирует параметр CFG (Classifier-Free Guidance) scale?


---

### Вопрос: Seed в генеративных моделях
**Вопрос:** Для чего используется seed в Stable Diffusion?

